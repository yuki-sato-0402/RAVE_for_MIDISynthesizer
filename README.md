# RAVE_for_MIDISynthesizer
Real-time MIDI-Synthesizer powered by the [RAVE](https://github.com/acids-ircam/RAVE) model and the [anira](https://github.com/anira-project/anira/tree/main) library. 

In the standard RAVE model, the input signal is the audio input from a microphone. However, in this application, the input signal is a sine wave generated by MIDI input. Additionally, the bias of the eight latent variables can be controlled via MIDI input.   
For details, please refer to the [Keyboard Operation Guide](https://github.com/yuki-sato-0402/RAVE_for_MIDISynthesizer/edit/main/README.md#keyboard-operation-guide).

The source code was written with reference to the [juce-audio-plugin](https://github.com/anira-project/anira/tree/main/examples/juce-audio-plugin) included in the anira library.

## Demonstration
[Youtube<img width="968" height="594" alt="Screenshot 2025-09-12 at 0 21 45" src="https://github.com/user-attachments/assets/13ef2e9e-ffbd-4b6c-9915-387028167876" />](https://youtu.be/ORYQNKNNJjU?si=6QDs7ne-mZo-zgeL) 

## Keyboard Operation Guide
Using a MIDI keyboard, you can sequentially control the frequency of the sine wave input to the RAVE model and the bias of eight latent variables. The first key pressed controls the frequency of the sine wave (hereafter referred to as the root note). Subsequent keys control the bias, but only function while the root note is held down. In other words, releasing the root note once starts the step where the root note is pressed again.

The bias amount can be controlled by the distance from the root note. Positions below the root are negative, and positions above the root are positive. A bias of 2.5 can be applied up to one octave away from the root. (This is like applying a 2.5/12 bias for every 12 semitones in an octave.)

The bias is controlled by releasing a note once and then controlling the next latent variable. After all eight latent variables have been specified, the system returns to the first.

In addition to the bias, you can also use the dial to scale (multiply) the latent variable from -1 to 1. Regarding scaling, I‚Äôm also considering whether it can be controlled using a physical controller such as a MIDI keyboard.

## üõ†Ô∏è Build Instructions
```
cd RAVE_for_MIDISynthesiser
git submodule update --init --recursive
cd build
cmake ..
cmake --build .
```

## About Pre-trained Model
In creating this project, I trained three types of RAVE models (water sounds, violin, and alto saxophone).   
You can download the models [here](https://www.dropbox.com/scl/fo/454ud8t5eulpj42d0ztmg/AB4ozeBlznhIaZ2UwxocFqg?rlkey=15en2vvl7gwtlahsg7lgmdnq5&st=g60m3rzv&dl=0).  

All three models were trained using the v1 architecture. While it may also depend on the machine's performance, the v2 architecture model frequently experienced audio dropouts and was quite unstable.

In Anira, the [InferenceConfig class](https://anira-project.github.io/anira/usage.html#inferenceconfig) allows you to specify the maximum inference time during initialization, making this adjustment particularly important.

## About the Number of Dimensions of Latent Variables
When training with the v1 architecture, the default number of dimensions for latent variables is 128. Since it would be difficult to control these individually, this application compresses the model to 8 dimensions before use. The number 8 is based on the original [RAVE VST](https://forum.ircam.fr/projects/detail/rave-vst/).  

The dimension of latent variables can be specified when exporting the model's checkpoint file as a torchscript (.ts). Please refer to [this](https://github.com/victor-shepardson/RAVE/blob/vs-ups/scripts/export.py) program.  

To check the number of dimensions in the model, please refer to [this](https://github.com/yuki-sato-0402/RAVE_for_MIDISynthesizer/blob/main/ConfirmingModelInformation/confirmLatentSpace.py) program. To check the minimum and maximum values of the latent variables, please refer to [this](https://github.com/yuki-sato-0402/RAVE_for_MIDISynthesizer/blob/main/ConfirmingModelInformation/confirmRangeRangeOfLatentVariables.py) program.

## About Training Data
For training the above models, the following datasets were used:
- [bpiyush/sound-of-water](https://huggingface.co/datasets/bpiyush/sound-of-water)

- [String Instruments Dataset](https://www.kaggle.com/datasets/aashnaashahh1504/string-instruments-dataset)

- [Real Saxophone Recordings for Audio-to-Score Music Transcription](https://grfia.dlsi.ua.es/audio-to-score/)


## Other References
- [nn-inference-template](https://github.com/Torsion-Audio/nn-inference-template)

- [RAVEv2_Training.ipynb](https://colab.research.google.com/drive/1ih-gv1iHEZNuGhHPvCHrleLNXvooQMvI?usp=sharing)

- [Tutorial: Build a MIDI synthesiser](https://juce.com/tutorials/tutorial_synth_using_midi_input/)

- [Let's build a synthesizer plug-in with C++ and the JUCE Framework!](https://youtube.com/playlist?list=PLLgJJsrdwhPwJimt5vtHtNmu63OucmPck&si=vfKCEvMZtt56co4B)

## Planned Features
- Consider a more intuitive method of operation.

- Allows you to select oscillators other than sine waves as input signals.

- Provide a button that opens a file dialog and allows you to select a model.
